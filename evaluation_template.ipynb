{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# Knapsack Solver Evaluation\n",
    "\n",
    "**Objective:** Evaluate all solver results from `solver_runs.json` according to the protocol in `team_project_plan.md`.\n",
    "\n",
    "**Protocol Steps:**\n",
    "1.  Load all solver run data (`solver_runs.json`) into a pandas DataFrame.\n",
    "2.  Determine the **Reference Value ($V_{known}$)** for all 27 instances.\n",
    "    * For `n=50`, $V_{known}$ is the value from the exact solvers (DP, B&B).\n",
    "    * For `n=200` & `n=500`, $V_{known}$ is the best value found across *all* algorithms and runs.\n",
    "3.  Calculate the **Optimality Gap** for every single run: `gap = (V_known - V_run) / V_known`.\n",
    "4.  Aggregate per-instance stats (for randomized algorithms) into `df_agg_instance`.\n",
    "5.  Aggregate per-algorithm/config stats (median gap, IQR, etc.) into `df_summary`.\n",
    "6.  Generate visualizations (boxplots for gap and runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from scipy.stats import iqr\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_ROOT = Path(\"knapsack_multisize_data\")\n",
    "RESULTS_DIR = Path(\"experiment_results\")\n",
    "PLOTS_DIR = RESULTS_DIR / \"evaluation_plots\"\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "RUNS_FILE = RESULTS_DIR / \"solver_runs.json\"\n",
    "\n",
    "# --- IMPORTANT: Set your exact solver names here ---\n",
    "# These names MUST match the 'method' names in solver_runs.json\n",
    "# (as defined in the 'SOLVERS_TO_RUN' dict in the previous notebook)\n",
    "# UPDATED: Added 'Backtracking' as per the project plan.\n",
    "EXACT_SOLVERS = [\n",
    "    \"DynamicProgramming\", \n",
    "    \"BranchAndBound\",\n",
    "    \"Backtracking\"\n",
    "]\n",
    "\n",
    "print(f\"Loading runs from: {RUNS_FILE}\")\n",
    "print(f\"Saving plots to:   {PLOTS_DIR}\")\n",
    "print(f\"Exact solvers defined as: {EXACT_SOLVERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file with all solver runs\n",
    "try:\n",
    "    df_runs = pd.read_json(RUNS_FILE)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: Could not load {RUNS_FILE}. {e}\")\n",
    "    print(\"Please ensure 'solver_framework.ipynb' (or similar) was run successfully.\")\n",
    "    # Create empty DataFrame to avoid downstream errors\n",
    "    df_runs = pd.DataFrame(columns=[\n",
    "        'method', 'parameters', 'seed', 'run_index', 'instance_file',\n",
    "        'instance_n', 'instance_dist', 'instance_cap_ratio_str',\n",
    "        'instance_seed', 'best_value', 'runtime', 'logs'\n",
    "    ])\n",
    "\n",
    "if not df_runs.empty:\n",
    "    print(f\"Loaded {len(df_runs)} total runs across {df_runs['method'].nunique()} methods.\")\n",
    "    \n",
    "    # --- Data Preprocessing ---\n",
    "    \n",
    "    # Check for and report any errors that occurred during the run\n",
    "    # Errors were saved as 'best_value': None\n",
    "    errors = df_runs['best_value'].isna().sum()\n",
    "    if errors > 0:\n",
    "        print(f\"\\nWARNING: Found {errors} runs that failed (had 'best_value' = None). These will be dropped.\")\n",
    "        print(df_runs[df_runs['best_value'].isna()][['method', 'instance_file', 'logs']])\n",
    "        df_runs = df_runs[df_runs['best_value'].notna()] # Drop failed runs\n",
    "    \n",
    "    # Convert columns to categories for efficient filtering/grouping\n",
    "    df_runs['instance_n'] = df_runs['instance_n'].astype('category')\n",
    "    df_runs['instance_dist'] = df_runs['instance_dist'].astype('category')\n",
    "    df_runs['method'] = df_runs['method'].astype('category')\n",
    "\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df_runs.info()\n",
    "    \n",
    "    print(\"\\nDataFrame Head:\")\n",
    "    display(df_runs.head())\n",
    "else:\n",
    "    print(\"\\nDataFrame is empty. Cannot proceed with evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vknown-md",
   "metadata": {},
   "source": [
    "## 2. Determine Reference Values ($V_{known}$)\n",
    "\n",
    "Here we establish the ground-truth optimal or best-known value for each of the 27 instances based on the evaluation protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vknown-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_known_map = pd.Series(dtype=float)\n",
    "\n",
    "if not df_runs.empty:\n",
    "    # --- Step 1: Get V_known for n=50 from exact solvers ---\n",
    "    df_n50 = df_runs[df_runs['instance_n'] == 50]\n",
    "    df_n50_exact = df_n50[df_n50['method'].isin(EXACT_SOLVERS)]\n",
    "    \n",
    "    if df_n50_exact.empty:\n",
    "        print(\"WARNING: No data found for n=50 using 'EXACT_SOLVERS'. Check your solver names.\")\n",
    "        print(\"Using best-known value from *all* n=50 solvers as a fallback.\")\n",
    "        v_known_n50 = df_n50.groupby('instance_file')['best_value'].max()\n",
    "    else:\n",
    "        v_known_n50 = df_n50_exact.groupby('instance_file')['best_value'].max()\n",
    "        print(f\"Found {len(v_known_n50)} reference values for n=50 from exact solvers.\")\n",
    "        \n",
    "        # --- Optional Verification: Check if exact solvers agree ---\n",
    "        if len(EXACT_SOLVERS) > 1:\n",
    "            check = df_n50_exact.groupby('instance_file')['best_value'].nunique()\n",
    "            if (check > 1).any():\n",
    "                print(\"\\n!! VERIFICATION WARNING: Your exact solvers DISAGREE on n=50 !!\")\n",
    "                disagree_files = check[check > 1].index\n",
    "                print(df_n50_exact[df_n50_exact['instance_file'].isin(disagree_files)][['method', 'instance_file', 'best_value']])\n",
    "            else:\n",
    "                print(\"Verification: All exact solvers agree on n=50 values. Good.\")\n",
    "\n",
    "    # --- Step 2: Get V_known for n=200 & n=500 from *all* runs ---\n",
    "    df_n_large = df_runs[df_runs['instance_n'].isin([200, 500])]\n",
    "    v_known_n_large = df_n_large.groupby('instance_file')['best_value'].max()\n",
    "    print(f\"Found {len(v_known_n_large)} best-known values for n=200 & n=500 from all runs.\")\n",
    "\n",
    "    # --- Step 3: Combine into one map and merge into main DataFrame ---\n",
    "    v_known_map = pd.concat([v_known_n50, v_known_n_large])\n",
    "    v_known_map.name = \"v_known\"\n",
    "    \n",
    "    df_runs = df_runs.merge(v_known_map, left_on='instance_file', right_index=True)\n",
    "    \n",
    "    print(f\"\\nTotal instances with reference values: {df_runs['instance_file'].nunique()}\")\n",
    "    display(v_known_map.to_frame().head())\n",
    "else:\n",
    "    print(\"DataFrame empty, skipping $V_{known}$ calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gap-md",
   "metadata": {},
   "source": [
    "## 3. Calculate Optimality Gap (Per-Run)\n",
    "\n",
    "Now we compute the relative gap for every single solver run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'v_known' in df_runs.columns:\n",
    "    # Calculate gap: (V_known - V_run) / V_known\n",
    "    numerator = df_runs['v_known'] - df_runs['best_value']\n",
    "    denominator = df_runs['v_known']\n",
    "    \n",
    "    # Handle V_known == 0 case to avoid division by zero\n",
    "    df_runs['gap'] = np.where(\n",
    "        denominator == 0,\n",
    "        0.0, # If V_known is 0, gap is 0\n",
    "        numerator / denominator\n",
    "    )\n",
    "    \n",
    "    # Handle any other weirdness (e.g., V_run > V_known, which would be a negative gap)\n",
    "    df_runs['gap'] = df_runs['gap'].fillna(0.0)\n",
    "    \n",
    "    print(\"Gap calculation complete. Descriptive stats for 'gap':\")\n",
    "    display(df_runs['gap'].describe())\n",
    "    \n",
    "    # Check for negative gaps (solver found a *better* value than V_known)\n",
    "    # This should only happen if an exact solver failed or wasn't run on n=50\n",
    "    if (df_runs['gap'] < -1e-9).any(): # Use tolerance for float precision\n",
    "        print(\"\\n!! WARNING: Found runs with a negative gap (better than V_known) !!\")\n",
    "        display(df_runs[df_runs['gap'] < -1e-9][[\n",
    "            'method', 'instance_file', 'instance_n', 'best_value', 'v_known', 'gap'\n",
    "        ]])\n",
    "else:\n",
    "    print(\"Column 'v_known' not found. Skipping gap calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agg-instance-md",
   "metadata": {},
   "source": [
    "## 4. Aggregate Per-Instance Statistics\n",
    "\n",
    "Here we collapse the 10 runs for randomized algorithms into summary statistics (mean, median, best, std) *for each instance*. Deterministic solvers will just have 1 run, so their std will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agg-instance-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_instance = pd.DataFrame()\n",
    "\n",
    "if 'gap' in df_runs.columns:\n",
    "    # Define the columns to group by for a unique (solver, instance) pair\n",
    "    group_cols = [\n",
    "        'method', 'parameters', 'instance_file', \n",
    "        'instance_n', 'instance_dist', 'instance_cap_ratio_str'\n",
    "    ]\n",
    "\n",
    "    # Define the aggregations\n",
    "    aggregations = {\n",
    "        # Gap stats\n",
    "        'gap': ['mean', 'median', 'std', \n",
    "                ('best', 'min'), # 'best' gap is the minimum gap\n",
    "                ('worst', 'max')], \n",
    "        \n",
    "        # Value stats\n",
    "        'best_value': ['mean', 'median', 'std', \n",
    "                       ('best', 'max'), # 'best' value is the maximum value\n",
    "                       ('worst', 'min')],\n",
    "        \n",
    "        # Runtime stats\n",
    "        'runtime': ['mean', 'median', 'std', 'sum'],\n",
    "        \n",
    "        # Count of runs\n",
    "        'run_index': ['count']\n",
    "    }\n",
    "\n",
    "    df_agg_instance = df_runs.groupby(group_cols).agg(aggregations)\n",
    "\n",
    "    # Flatten the MultiIndex columns (e.g., ('gap', 'mean') -> 'gap_mean')\n",
    "    df_agg_instance.columns = ['_'.join(col).strip() for col in df_agg_instance.columns.values]\n",
    "    \n",
    "    # Rename 'gap_best' to 'gap_best_run' to avoid confusion with 'best_value_best_run'\n",
    "    df_agg_instance = df_agg_instance.rename(columns={\n",
    "        'gap_best': 'gap_best_run',\n",
    "        'gap_worst': 'gap_worst_run',\n",
    "        'best_value_best': 'best_value_best_run',\n",
    "        'best_value_worst': 'best_value_worst_run',\n",
    "        'run_index_count': 'n_runs'\n",
    "    })\n",
    "\n",
    "    # Fill std=NaN with 0 (for deterministic solvers with 1 run)\n",
    "    std_cols = [col for col in df_agg_instance.columns if col.endswith('_std')]\n",
    "    df_agg_instance[std_cols] = df_agg_instance[std_cols].fillna(0.0)\n",
    "\n",
    "    df_agg_instance = df_agg_instance.reset_index()\n",
    "\n",
    "    print(f\"Created per-instance aggregate table with {len(df_agg_instance)} rows.\")\n",
    "    display(df_agg_instance.head())\n",
    "else:\n",
    "    print(\"Column 'gap' not found. Skipping instance aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agg-algo-md",
   "metadata": {},
   "source": [
    "## 5. Final Summary Table (Per-Algorithm)\n",
    "\n",
    "This is the final report. We aggregate the per-instance statistics to get a high-level summary for each algorithm across each (n, distribution) pair.\n",
    "\n",
    "We report **median $\\pm$ IQR** and **mean $\\pm$ std** for the **median gap** (i.e., the `gap_median` from the table above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agg-algo-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame()\n",
    "\n",
    "if not df_agg_instance.empty:\n",
    "    # Group by method, size (n), and distribution\n",
    "    summary_group_cols = ['method', 'instance_n', 'instance_dist']\n",
    "\n",
    "    # Define aggregations for the final summary table\n",
    "    summary_aggregations = {\n",
    "        # Stats on the 'median_gap' from the 10 runs\n",
    "        'gap_median': ['mean', 'std', 'median', iqr],\n",
    "        \n",
    "        # Stats on the 'best_gap' from the 10 runs\n",
    "        'gap_best_run': ['mean', 'median'],\n",
    "        \n",
    "        # Stats on the 'mean_runtime' from the 10 runs\n",
    "        'runtime_mean': ['mean', 'std', 'median', 'sum']\n",
    "    }\n",
    "\n",
    "    df_summary = df_agg_instance.groupby(summary_group_cols).agg(summary_aggregations)\n",
    "\n",
    "    # Flatten column names (e.g., 'gap_median_mean')\n",
    "    df_summary.columns = ['_'.join(col).strip() for col in df_summary.columns.values]\n",
    "    \n",
    "    # Rename for clarity\n",
    "    df_summary = df_summary.rename(columns={\n",
    "        'gap_median_mean': 'gap_median_of_medians',\n",
    "        'gap_median_std': 'gap_std_of_medians',\n",
    "        'gap_median_median': 'gap_median_of_medians_final',\n",
    "        'gap_median_iqr': 'gap_iqr_of_medians'\n",
    "    })\n",
    "    \n",
    "    df_summary = df_summary.reset_index()\n",
    "\n",
    "    print(\"--- Final Evaluation Summary ---\\n\")\n",
    "    \n",
    "    # Display with nice formatting\n",
    "    float_format = \"{:.5f}\".format\n",
    "    display(df_summary.style.format({\n",
    "        'gap_median_of_medians': float_format,\n",
    "        'gap_std_of_medians': float_format,\n",
    "        'gap_median_of_medians_final': float_format,\n",
    "        'gap_iqr_of_medians': float_format,\n",
    "        'gap_best_run_mean': float_format,\n",
    "        'gap_best_run_median': float_format,\n",
    "        'runtime_mean_mean': \"{:.3f}\".format,\n",
    "        'runtime_mean_std': \"{:.3f}\".format,\n",
    "        'runtime_mean_median': \"{:.3f}\".format,\n",
    "        'runtime_mean_sum': \"{:.1f}\".format\n",
    "    }))\n",
    "    \n",
    "    # --- Save Summary to CSV ---\n",
    "    summary_path = RESULTS_DIR / \"evaluation_summary.csv\"\n",
    "    try:\n",
    "        df_summary.to_csv(summary_path, index=False, float_format=\"%.6f\")\n",
    "        print(f\"\\nSummary table saved to {summary_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving summary CSV: {e}\")\n",
    "else:\n",
    "    print(\"Aggregated instance DataFrame is empty. Skipping final summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-gap-md",
   "metadata": {},
   "source": [
    "## 6. Visualization: Optimality Gap\n",
    "\n",
    "Visualize the distribution of optimality gaps for each solver. We will plot the **best gap** (`gap_best_run`) achieved for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-gap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_agg_instance.empty:\n",
    "    print(\"Generating Optimality Gap boxplots...\")\n",
    "    \n",
    "    # Use seaborn's catplot (kind='box') for a faceted view\n",
    "    g = sns.catplot(\n",
    "        data=df_agg_instance,\n",
    "        x='method',\n",
    "        y='gap_best_run', # Plotting the 'best run' gap per instance\n",
    "        col='instance_dist',  # Separate columns by distribution\n",
    "        row='instance_n',     # Separate rows by size (n)\n",
    "        kind='box',\n",
    "        height=4, \n",
    "        aspect=1.5,\n",
    "        margin_titles=True,\n",
    "        sharey=False # Gaps might be on very different scales\n",
    "    )\n",
    "\n",
    "    # Use a symmetric log scale for 'gap' because it can be 0 or very close to 0\n",
    "    g.set(yscale=\"symlog\", linthresh=1e-5)\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    g.set_axis_labels(\"Solver Method\", \"Best Optimality Gap (symlog scale)\")\n",
    "    g.fig.suptitle(\"Optimality Gap (Best Run) by Solver, Size, and Distribution\", y=1.03)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = PLOTS_DIR / \"gap_boxplot_faceted.png\"\n",
    "    g.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Gap plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame empty. Skipping gap visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-runtime-md",
   "metadata": {},
   "source": [
    "## 7. Visualization: Runtime\n",
    "\n",
    "Visualize the distribution of execution time for each solver. We will plot the **mean runtime** (`runtime_mean`) on a **log scale**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-runtime-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_agg_instance.empty:\n",
    "    print(\"Generating Runtime boxplots...\")\n",
    "    \n",
    "    g = sns.catplot(\n",
    "        data=df_agg_instance,\n",
    "        x='method',\n",
    "        y='runtime_mean', # Plotting the 'mean run' time per instance\n",
    "        col='instance_dist', \n",
    "        row='instance_n', \n",
    "        kind='box',\n",
    "        height=4, \n",
    "        aspect=1.5,\n",
    "        margin_titles=True,\n",
    "        sharey=False # Runtimes will vary greatly by n\n",
    "    )\n",
    "\n",
    "    # Use a log scale for runtime, as it spans orders of magnitude\n",
    "    g.set(yscale=\"log\") \n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    g.set_axis_labels(\"Solver Method\", \"Mean Runtime (log scale, seconds)\")\n",
    "    g.fig.suptitle(\"Mean Solver Runtime by Size and Distribution\", y=1.03)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = PLOTS_DIR / \"runtime_boxplot_faceted.png\"\n",
    "    g.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Runtime plot saved to {plot_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame empty. Skipping runtime visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-convergence-md",
   "metadata": {},
   "source": [
    "## 8. (Placeholder) Visualization: Convergence Plots\n",
    "\n",
    "This section is a **placeholder** to show how you *would* plot convergence, as requested in the plan.\n",
    "\n",
    "It assumes that your randomized solvers (GA, SA, ACO, etc.) stored an array or list called `convergence_history` inside the `logs` dictionary for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-convergence-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This is a template, you must customize it ---\n",
    "\n",
    "# 1. Define the solver and instance you want to inspect\n",
    "SOLVER_TO_PLOT = \"GeneticAlgorithm\" # <-- CHANGE THIS to your solver name\n",
    "INSTANCE_FILE_TO_PLOT = \"knapsack_n500_seed20251025.json\" # <-- CHANGE THIS to an instance file\n",
    "\n",
    "if 'gap' not in df_runs.columns:\n",
    "    print(\"df_runs not available. Skipping convergence plot.\")\n",
    "else:\n",
    "    # 2. Filter df_runs to get the 10 runs for this (solver, instance) pair\n",
    "    df_plot = df_runs[\n",
    "        (df_runs['method'] == SOLVER_TO_PLOT) &\n",
    "        (df_runs['instance_file'] == INSTANCE_FILE_TO_PLOT)\n",
    "    ]\n",
    "\n",
    "    if df_plot.empty:\n",
    "        print(f\"No data found for {SOLVER_TO_PLOT} on {INSTANCE_FILE_TO_PLOT}\")\n",
    "        print(\"Check your solver names and instance file names.\")\n",
    "    else:\n",
    "        print(f\"Found {len(df_plot)} runs to plot for {SOLVER_TO_PLOT}...\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 7))\n",
    "        \n",
    "        v_known_for_instance = df_plot['v_known'].iloc[0]\n",
    "        \n",
    "        # 3. Loop over each run and plot its history\n",
    "        for idx, run in df_plot.iterrows():\n",
    "            # 4. *** This is the critical part ***\n",
    "            # You must know the key inside your 'logs' dict.\n",
    "            # We assume it's 'convergence_history' for this example.\n",
    "            history = run['logs'].get('convergence_history') \n",
    "            \n",
    "            if history and isinstance(history, list):\n",
    "                # Plot value\n",
    "                plt.plot(history, label=f\"Run {run['run_index']}\", alpha=0.6)\n",
    "                # To plot gap instead:\n",
    "                # gap_history = [(v_known_for_instance - v) / v_known_for_instance for v in history]\n",
    "                # plt.plot(gap_history, label=f\"Run {run['run_index']}\", alpha=0.6)\n",
    "            else:\n",
    "                print(f\"  -> Run {run['run_index']}: 'convergence_history' key not found or not a list in logs.\")\n",
    "        \n",
    "        # Plot the known best value as a dashed line\n",
    "        plt.axhline(y=v_known_for_instance, color='red', linestyle='--', label=f\"V_known = {v_known_for_instance}\")\n",
    "        \n",
    "        plt.title(f\"Convergence Plot: {SOLVER_TO_PLOT}\\nInstance: {INSTANCE_FILE_TO_PLOT}\")\n",
    "        plt.xlabel(\"Iteration / Generation\")\n",
    "        plt.ylabel(\"Best Value Found\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = PLOTS_DIR / f\"convergence_{SOLVER_TO_PLOT}_n{df_plot['instance_n'].iloc[0]}.png\"\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Convergence plot saved to {plot_path}\")\n",
    "        \n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
