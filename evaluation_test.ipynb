{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0fabb3",
   "metadata": {},
   "source": [
    "# Merge Solver Results and Identify Best Algorithms\n",
    "\n",
    "This notebook loads the results from `quantum_solver_runs.json` and `solver_runs.json`, merges them, and identifies for each instance which algorithm gave the best optimal solution.\n",
    "\n",
    "**Outputs:**\n",
    "- `merged_best_by_instance.csv`\n",
    "- `merged_best_by_instance.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d67950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9e942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = Path(\"experiment_results\")\n",
    "# Use the / operator to join the Path object with the string\n",
    "QUANTUM_PATH = RESULTS_DIR / \"quantum_solver_runs.json\"\n",
    "CLASSICAL_PATH = RESULTS_DIR / \"solver_runs.json\"\n",
    "OUT_CSV = RESULTS_DIR / \"merged_best_by_instance.csv\"\n",
    "OUT_JSON = RESULTS_DIR / \"merged_best_by_instance.json\"\n",
    "EXACT_PATH = RESULTS_DIR / \"exact_solver_runs.json\"\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3d453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_value(val):\n",
    "    try:\n",
    "        if val is None:\n",
    "            return None\n",
    "        if isinstance(val, (int, float)):\n",
    "            if math.isnan(val) or not math.isfinite(val):\n",
    "                return None\n",
    "            return float(val)\n",
    "        v = float(val)\n",
    "        if math.isnan(v) or not math.isfinite(v):\n",
    "            return None\n",
    "        return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def collect_runs(*lists_of_runs):\n",
    "    runs_by_instance = defaultdict(list)\n",
    "    for lst in lists_of_runs:\n",
    "        for r in lst:\n",
    "            inst = r.get(\"instance_file\") or r.get(\"instance\")\n",
    "            if not inst:\n",
    "                continue\n",
    "            bv_raw = r.get(\"best_value\")\n",
    "            bv = safe_value(bv_raw)\n",
    "            rec = {\n",
    "                \"method\": r.get(\"method\"),\n",
    "                \"best_value\": bv,\n",
    "                \"best_value_raw\": bv_raw,\n",
    "                \"runtime\": r.get(\"runtime\"),\n",
    "                \"seed\": r.get(\"seed\"),\n",
    "                \"run_index\": r.get(\"run_index\"),\n",
    "                \"parameters\": r.get(\"parameters\"),\n",
    "                \"instance_n\": r.get(\"instance_n\"),\n",
    "                \"instance_dist\": r.get(\"instance_dist\"),\n",
    "                \"instance_cap_ratio_str\": r.get(\"instance_cap_ratio_str\"),\n",
    "                \"instance_seed\": r.get(\"instance_seed\"),\n",
    "            }\n",
    "            runs_by_instance[inst].append(rec)\n",
    "    return runs_by_instance\n",
    "\n",
    "def pick_best_for_each_instance(runs_by_instance):\n",
    "    results = []\n",
    "    for inst, runs in runs_by_instance.items():\n",
    "        max_val = None\n",
    "        for r in runs:\n",
    "            bv = r[\"best_value\"]\n",
    "            if bv is None:\n",
    "                continue\n",
    "            if max_val is None or bv > max_val:\n",
    "                max_val = bv\n",
    "\n",
    "        winners = []\n",
    "        if max_val is not None:\n",
    "            for r in runs:\n",
    "                if r[\"best_value\"] is not None and r[\"best_value\"] == max_val:\n",
    "                    winners.append({\n",
    "                        \"method\": r[\"method\"],\n",
    "                        \"best_value\": r[\"best_value\"],\n",
    "                        \"runtime\": r[\"runtime\"],\n",
    "                        \"seed\": r[\"seed\"],\n",
    "                        \"run_index\": r[\"run_index\"],\n",
    "                        \"parameters\": r[\"parameters\"]\n",
    "                    })\n",
    "\n",
    "        by_method_type = defaultdict(list)\n",
    "        for r in runs:\n",
    "            m = r.get(\"method\", \"\")\n",
    "            if \"Quantum\" in (m or \"\") or \"quantum\" in (m or \"\") or m in (\"QAOA\", \"VQE\", \"QuantumAnnealing\"):\n",
    "                key = \"quantum\"\n",
    "            else:\n",
    "                key = \"classical\"\n",
    "            by_method_type[key].append(r)\n",
    "\n",
    "        def best_in_group(group):\n",
    "            mv = None\n",
    "            for rr in group:\n",
    "                if rr[\"best_value\"] is None:\n",
    "                    continue\n",
    "                if mv is None or rr[\"best_value\"] > mv:\n",
    "                    mv = rr[\"best_value\"]\n",
    "            return mv\n",
    "\n",
    "        best_quantum = best_in_group(by_method_type.get(\"quantum\", []))\n",
    "        best_classical = best_in_group(by_method_type.get(\"classical\", []))\n",
    "\n",
    "        results.append({\n",
    "            \"instance_file\": inst,\n",
    "            \"instance_n\": runs[0].get(\"instance_n\"),\n",
    "            \"instance_dist\": runs[0].get(\"instance_dist\"),\n",
    "            \"instance_cap_ratio_str\": runs[0].get(\"instance_cap_ratio_str\"),\n",
    "            \"best_value_overall\": max_val,\n",
    "            \"winners\": winners,\n",
    "            \"best_quantum\": best_quantum,\n",
    "            \"best_classical\": best_classical,\n",
    "            \"num_runs\": len(runs)\n",
    "        })\n",
    "\n",
    "    results.sort(key=lambda x: x[\"instance_file\"])\n",
    "    return results\n",
    "\n",
    "def write_csv(results, path):\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\n",
    "            \"instance_file\", \"instance_n\", \"instance_dist\", \"instance_cap_ratio_str\",\n",
    "            \"best_value_overall\", \"num_winners\", \"winner_methods\",\n",
    "            \"best_quantum\", \"best_classical\", \"num_runs\"\n",
    "        ]\n",
    "        writer.writerow(header)\n",
    "        for r in results:\n",
    "            winner_methods = \";\".join(sorted({w[\"method\"] for w in r[\"winners\"]})) if r[\"winners\"] else \"\"\n",
    "            writer.writerow([\n",
    "                r[\"instance_file\"],\n",
    "                r.get(\"instance_n\"),\n",
    "                r.get(\"instance_dist\"),\n",
    "                r.get(\"instance_cap_ratio_str\"),\n",
    "                r.get(\"best_value_overall\"),\n",
    "                len(r[\"winners\"]),\n",
    "                winner_methods,\n",
    "                r.get(\"best_quantum\"),\n",
    "                r.get(\"best_classical\"),\n",
    "                r.get(\"num_runs\")\n",
    "            ])\n",
    "\n",
    "def write_json(results, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8884e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 81 quantum runs and 324 classical runs.\n",
      "Found 27 unique instances.\n",
      "\n",
      "Results saved to:\n",
      " - C:\\Users\\abhay\\Desktop\\Projects\\COMA_IIITR\\experiment_results\\merged_best_by_instance.csv\n",
      " - C:\\Users\\abhay\\Desktop\\Projects\\COMA_IIITR\\experiment_results\\merged_best_by_instance.json\n",
      "\n",
      "Preview:\n",
      "- knapsack_n100_seed20251020.json: best=25554.0 by ['ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251023.json: best=39931.0 by ['Greedy_Ratio', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251026.json: best=45613.0 by ['Greedy_Ratio', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251120.json: best=18691.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251123.json: best=33418.0 by ['GeneticAlgorithm', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251126.json: best=43669.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251220.json: best=2064.0 by ['ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251223.json: best=1332.0 by ['Greedy_Ratio', 'GeneticAlgorithm', 'Greedy_Value', 'AntColonyOptimization', 'AntColonyOptimization', 'AntColonyOptimization', 'Greedy_Weight', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n100_seed20251226.json: best=2194.0 by ['Greedy_Ratio', 'AntColonyOptimization', 'AntColonyOptimization', 'AntColonyOptimization', 'Greedy_Weight', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251021.json: best=63432.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251024.json: best=103886.0 by ['Greedy_Ratio']\n",
      "- knapsack_n250_seed20251027.json: best=121865.0 by ['Greedy_Ratio']\n",
      "- knapsack_n250_seed20251121.json: best=41378.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251124.json: best=84789.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251127.json: best=113622.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251221.json: best=2125.0 by ['Greedy_Ratio', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251224.json: best=3954.0 by ['Greedy_Ratio', 'AntColonyOptimization', 'AntColonyOptimization', 'AntColonyOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n250_seed20251227.json: best=1644.0 by ['AntColonyOptimization', 'AntColonyOptimization', 'AntColonyOptimization']\n",
      "- knapsack_n500_seed20251022.json: best=117410.0 by ['Greedy_Ratio']\n",
      "- knapsack_n500_seed20251025.json: best=205859.0 by ['Greedy_Ratio']\n",
      "- knapsack_n500_seed20251028.json: best=237590.0 by ['Greedy_Ratio']\n",
      "- knapsack_n500_seed20251122.json: best=87393.0 by ['ParticleSwarmOptimization']\n",
      "- knapsack_n500_seed20251125.json: best=168628.0 by ['Greedy_Ratio']\n",
      "- knapsack_n500_seed20251128.json: best=226674.0 by ['Greedy_Ratio', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n500_seed20251222.json: best=7097.0 by ['Greedy_Ratio', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n",
      "- knapsack_n500_seed20251225.json: best=7252.0 by ['Greedy_Ratio', 'AntColonyOptimization', 'AntColonyOptimization']\n",
      "- knapsack_n500_seed20251228.json: best=7445.0 by ['Greedy_Ratio', 'AntColonyOptimization', 'AntColonyOptimization', 'AntColonyOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization', 'ParticleSwarmOptimization']\n"
     ]
    }
   ],
   "source": [
    "q = load_json(QUANTUM_PATH)\n",
    "c = load_json(CLASSICAL_PATH)\n",
    "print(f\"Loaded {len(q)} quantum runs and {len(c)} classical runs.\")\n",
    "\n",
    "runs_by_instance = collect_runs(q, c)\n",
    "print(f\"Found {len(runs_by_instance)} unique instances.\")\n",
    "\n",
    "results = pick_best_for_each_instance(runs_by_instance)\n",
    "write_csv(results, OUT_CSV)\n",
    "write_json(results, OUT_JSON)\n",
    "\n",
    "print(f\"\\nResults saved to:\\n - {OUT_CSV.resolve()}\\n - {OUT_JSON.resolve()}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "for r in results:\n",
    "    print(f\"- {r['instance_file']}: best={r['best_value_overall']} by {[w['method'] for w in r['winners']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd181734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c2ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = RESULTS_DIR / \"plots\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you want to force maximize/minimize set this to True/False. If None, code auto-detects.\n",
    "FORCE_MAXIMIZE = None  # set True to force \"higher is better\", False to force \"lower is better\", None to auto-decide\n",
    "\n",
    "# Use percent gap if True, otherwise absolute gap.\n",
    "USE_PERCENT_GAP = True\n",
    "\n",
    "FIGSIZE = (14,6)\n",
    "DPI = 150\n",
    "FONT_TITLE = 14\n",
    "FONT_AXIS = 12\n",
    "FONT_ANN = 10\n",
    "BAR_HEIGHT = 0.6\n",
    "USE_PERCENT_GAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e9fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- helper loaders -----\n",
    "def safe_load_json(p: Path):\n",
    "    try:\n",
    "        with open(p,\"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def flatten_runs(obj_list, tag):\n",
    "    rows=[]\n",
    "    if not obj_list: return rows\n",
    "    for rec in obj_list:\n",
    "        inst = rec.get(\"instance_file\") or rec.get(\"instance\") or rec.get(\"problem\") or rec.get(\"inst\")\n",
    "        method = rec.get(\"method\") or rec.get(\"solver\") or rec.get(\"algorithm\") or rec.get(\"solver_name\") or rec.get(\"algo\")\n",
    "        # common value and capacity names\n",
    "        best_value = rec.get(\"best_value\") if \"best_value\" in rec else rec.get(\"value\") if \"value\" in rec else rec.get(\"objective\")\n",
    "        # capacity candidates in the run record (sometimes stored)\n",
    "        capacity_fields = [\"capacity\",\"used_capacity\",\"used_weight\",\"total_weight\",\"weight\",\"capacity_used\",\"knapsack_capacity\",\"capacity_value\",\"used_capacity_value\"]\n",
    "        cap = None\n",
    "        for k in capacity_fields:\n",
    "            if k in rec:\n",
    "                cap = rec.get(k); break\n",
    "        rows.append({\n",
    "            \"instance_file\": inst,\n",
    "            \"method\": method or \"unknown\",\n",
    "            \"best_value\": best_value,\n",
    "            \"capacity_in_run\": cap,\n",
    "            \"runtime\": rec.get(\"runtime\") or rec.get(\"time\") or rec.get(\"duration\"),\n",
    "            \"raw\": rec,\n",
    "            \"source\": tag\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7952a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Load data ==========\n",
    "merged = safe_load_json(OUT_JSON) or []\n",
    "classical = safe_load_json(CLASSICAL_PATH) or []\n",
    "quantum = safe_load_json(QUANTUM_PATH) or []\n",
    "\n",
    "# fallback: auto-detect JSON files in DATA_DIR if any of these are missing\n",
    "if not (merged or classical or quantum):\n",
    "    for p in RESULTS_DIR.glob(\"*.json\"):\n",
    "        j = safe_load_json(p)\n",
    "        if isinstance(j, list) and j:\n",
    "            keys = set(j[0].keys())\n",
    "            if keys & {\"method\",\"solver\",\"algorithm\",\"best_value\"}:\n",
    "                if not classical: classical = j\n",
    "                elif not quantum: quantum = j\n",
    "            elif keys & {\"best_value_overall\",\"instance_file\",\"best_overall\"}:\n",
    "                if not merged: merged = j\n",
    "\n",
    "rows = []\n",
    "rows += flatten_runs(classical, \"classical\")\n",
    "rows += flatten_runs(quantum, \"quantum\")\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No run records found. Check your JSON files or paths under /mnt/data.\")\n",
    "\n",
    "# normalize instance id (file stem)\n",
    "df['instance_id'] = df['instance_file'].apply(lambda s: Path(s).stem if isinstance(s,str) and s else str(s))\n",
    "\n",
    "# ----- try to extract capacity constraint per instance -----\n",
    "def find_capacity_from_merged(inst_stem, merged_list):\n",
    "    \"\"\"Check merged records for capacity-like keys.\"\"\"\n",
    "    if not merged_list: return None\n",
    "    for rec in merged_list:\n",
    "        # match by filename stem or by explicit instance id\n",
    "        inst_candidate = rec.get(\"instance_file\") or rec.get(\"instance\") or rec.get(\"problem\")\n",
    "        if not inst_candidate: continue\n",
    "        if Path(inst_candidate).stem != inst_stem: continue\n",
    "        # look for capacity-like keys\n",
    "        for key in [\"capacity_constraint\",\"knapsack_capacity\",\"capacity_total\",\"capacity\",\"cap\",\"capacity_limit\",\"W\",\"total_capacity\",\"best_capacity_overall\"]:\n",
    "            if key in rec:\n",
    "                return rec.get(key)\n",
    "        # sometimes merged only has 'instance' with nested info\n",
    "        if isinstance(rec, dict):\n",
    "            # search nested dict shallowly\n",
    "            for v in rec.values():\n",
    "                if isinstance(v, (int,float)):\n",
    "                    # not ideal; skip\n",
    "                    pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e5fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_capacity_in_instance_file(instance_file_path):\n",
    "    \"\"\"Open instance file if present and try to find capacity-like keys. Supports JSON, CSV, plain text.\"\"\"\n",
    "    p = Path(instance_file_path)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    # JSON\n",
    "    try:\n",
    "        j = json.loads(p.read_text())\n",
    "        if isinstance(j, dict):\n",
    "            for key in [\"capacity\",\"knapsack_capacity\",\"capacity_constraint\",\"cap\",\"W\",\"total_capacity\",\"capacity_limit\"]:\n",
    "                if key in j:\n",
    "                    return j[key]\n",
    "            # also check nested keys shallowly\n",
    "            for v in j.values():\n",
    "                if isinstance(v, (int,float)):\n",
    "                    # heuristics: ignore lists of items\n",
    "                    # skip this broad match to avoid false positives\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    # CSV: look for header with capacity-like name\n",
    "    try:\n",
    "        import csv\n",
    "        with open(p,\"r\") as f:\n",
    "            header = f.readline().strip().lower()\n",
    "            for token in [\"capacity\",\"knapsack_capacity\",\"cap\",\"capacity_limit\",\"total_capacity\"]:\n",
    "                if token in header:\n",
    "                    # read with pandas and pick first value of that column\n",
    "                    try:\n",
    "                        dfc = pd.read_csv(p)\n",
    "                        for key in dfc.columns:\n",
    "                            if token in str(key).lower():\n",
    "                                val = dfc[key].dropna().unique()\n",
    "                                if len(val)>0 and np.isscalar(val[0]):\n",
    "                                    return val[0]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Plain text fallback: scan for integer after \"capacity\" word\n",
    "    try:\n",
    "        text = p.read_text()\n",
    "        m = re.search(r'capacity[^0-9\\-]*([0-9]+(?:\\.[0-9]+)?)', text, re.IGNORECASE)\n",
    "        if m:\n",
    "            return float(m.group(1))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Build capacity_map\n",
    "capacity_map = {}\n",
    "for inst in df['instance_id'].unique():\n",
    "    # 1) check merged file\n",
    "    cap = find_capacity_from_merged(inst, merged)\n",
    "    if cap is not None:\n",
    "        capacity_map[inst] = cap\n",
    "        continue\n",
    "    # 2) check capacity present in any run record (capacity_in_run field)\n",
    "    cap_candidates = df.loc[df['instance_id']==inst, 'capacity_in_run'].dropna().unique().tolist()\n",
    "    if cap_candidates:\n",
    "        # choose numeric candidate if possible\n",
    "        for c in cap_candidates:\n",
    "            try:\n",
    "                cnum = float(c)\n",
    "                capacity_map[inst] = cnum\n",
    "                break\n",
    "            except Exception:\n",
    "                # if it's a string path or nested, ignore\n",
    "                pass\n",
    "        if inst in capacity_map:\n",
    "            continue\n",
    "    # 3) try to open referenced instance file(s) for that instance (if full path available in rows)\n",
    "    inst_rows = df[df['instance_id']==inst]\n",
    "    found = None\n",
    "    for ref in inst_rows['instance_file'].dropna().unique():\n",
    "        # try relative to DATA_DIR and the raw path\n",
    "        candidates = [Path(ref), RESULTS_DIR / Path(ref), RESULTS_DIR / Path(ref).name]\n",
    "        for cand in candidates:\n",
    "            cand = cand.resolve() if cand.exists() else cand\n",
    "            if cand.exists():\n",
    "                found = find_capacity_in_instance_file(cand)\n",
    "                if found is not None:\n",
    "                    capacity_map[inst] = found\n",
    "                    break\n",
    "        if inst in capacity_map:\n",
    "            break\n",
    "    # if not found, leave missing (we'll fallback later)\n",
    "    if inst not in capacity_map:\n",
    "        capacity_map[inst] = None\n",
    "\n",
    "# 4) final fallback: use maximum achieved capacity among runs for that instance (as a lower-quality substitute)\n",
    "for inst in df['instance_id'].unique():\n",
    "    if capacity_map.get(inst) is None:\n",
    "        vals = pd.to_numeric(df.loc[df['instance_id']==inst, 'capacity_in_run'], errors='coerce').dropna()\n",
    "        if not vals.empty:\n",
    "            capacity_map[inst] = vals.max()\n",
    "        else:\n",
    "            # try use best_overall from merged if available under other key names\n",
    "            # many merged entries may have 'best_value_overall' which is objective not capacity -> avoid using it by default\n",
    "            capacity_map[inst] = None\n",
    "\n",
    "# attach capacity constraint into dataframe\n",
    "df['capacity_constraint'] = df['instance_id'].map(capacity_map)\n",
    "# capacity used by this run (if available) else try to map best_value to capacity (some runs store capacity under 'best_value')\n",
    "df['capacity_used'] = pd.to_numeric(df['capacity_in_run'], errors='coerce')\n",
    "# if capacity_used NA, try best_value (but only if capacity_constraint exists to avoid mixing objective)\n",
    "mask_missing = df['capacity_used'].isna() & df['best_value'].notna()\n",
    "df.loc[mask_missing, 'capacity_used'] = pd.to_numeric(df.loc[mask_missing, 'best_value'], errors='coerce')\n",
    "\n",
    "# if capacity_used still NA, leave as NaN\n",
    "# compute best achieved per instance (use capacity_used if present)\n",
    "df['capacity_used'] = pd.to_numeric(df['capacity_used'], errors='coerce')\n",
    "best_by_instance = df.groupby('instance_id')['capacity_used'].max().rename('best_capacity_achieved')\n",
    "df = df.join(best_by_instance, on='instance_id')\n",
    "\n",
    "# compute percent of capacity used vs constraint (if constraint present)\n",
    "def percent_of_constraint(row):\n",
    "    cap = row['capacity_constraint']\n",
    "    if pd.isna(cap) or cap == 0:\n",
    "        return np.nan\n",
    "    return float(row['best_capacity_achieved']) / float(cap) * 100.0\n",
    "\n",
    "summary = df.groupby(['instance_id']).agg(\n",
    "    capacity_constraint=('capacity_constraint','first'),\n",
    "    best_capacity_achieved=('best_capacity_achieved','first')\n",
    ").reset_index()\n",
    "summary['pct_of_capacity'] = summary.apply(percent_of_constraint, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea60710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary CSV\n",
    "OUT_SUM = RESULTS_DIR / \"per_instance_capacity_summary.csv\"\n",
    "OUT_SUM.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary.to_csv(OUT_SUM, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc95e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 27 capacity plots to experiment_results\\plots\n",
      "Saved capacity summary CSV to: experiment_results\\per_instance_capacity_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ----- plotting: horizontal bar with capacity constraint marker -----\n",
    "saved = []\n",
    "for inst, sub in df.groupby('instance_id'):\n",
    "    sub2 = sub.copy()\n",
    "    # prefer rows with numeric capacity_used\n",
    "    sub2 = sub2[~sub2['capacity_used'].isna()].copy()\n",
    "    if sub2.empty:\n",
    "        # if no capacity_used values, skip instance but keep in summary\n",
    "        print(f\"[WARN] instance {inst}: no capacity_used numeric values found; skipping plot.\")\n",
    "        continue\n",
    "    sub_sorted = sub2.sort_values('capacity_used', ascending=False).reset_index(drop=True)\n",
    "    labels = [f\"{str(m)} [{s}]\" for m,s in zip(sub_sorted['method'], sub_sorted['source'])]\n",
    "    values = sub_sorted['capacity_used'].astype(float).values\n",
    "    cap_cons = sub_sorted['capacity_constraint'].dropna().unique()\n",
    "    cap_val = float(cap_cons[0]) if len(cap_cons)>0 else None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE, dpi=DPI)\n",
    "    y_pos = np.arange(len(values))\n",
    "    ax.barh(y_pos, values, height=BAR_HEIGHT)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels, fontsize=FONT_AXIS)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Capacity used (higher = better)\", fontsize=FONT_AXIS)\n",
    "    ax.set_title(f\"Instance {inst} — capacity used vs capacity constraint\", fontsize=FONT_TITLE)\n",
    "\n",
    "    # annotate each bar with gap to best (percent) and runtime if present\n",
    "    best_val = values.max()\n",
    "    for i,(val,rt,gap_src) in enumerate(zip(values, sub_sorted['runtime'].values, sub_sorted.get('gap_pct', np.zeros(len(values))))):\n",
    "        # compute gap relative to best_val\n",
    "        if best_val and best_val!=0:\n",
    "            gap_pct = (best_val - val) / best_val * 100.0\n",
    "        else:\n",
    "            gap_pct = 0.0\n",
    "        ann_gap = f\"{gap_pct:.2f}% gap\"\n",
    "        ann_rt = f\"{rt:.3g}s\" if not pd.isna(rt) else \"\"\n",
    "        ann = ann_gap + (f\" · {ann_rt}\" if ann_rt else \"\")\n",
    "        ax.text(val + max(1e-9, 0.01*max(values)), i, ann, va='center', fontsize=FONT_ANN)\n",
    "\n",
    "    # capacity constraint marker and annotation\n",
    "    if cap_val is not None and not pd.isna(cap_val):\n",
    "        ax.axvline(cap_val, color='k', linestyle='--', linewidth=1)\n",
    "        # annotate best achieved vs capacity as percent\n",
    "        pct = (best_val / cap_val * 100.0) if cap_val!=0 else np.nan\n",
    "        ax.text(cap_val, -0.5, f\"capacity constraint = {cap_val}\\nbest uses {pct:.2f}% of capacity\" if not np.isnan(pct) else f\"capacity constraint = {cap_val}\", \n",
    "                va='bottom', ha='center', fontsize=FONT_ANN, bbox=dict(facecolor='white', alpha=0.6))\n",
    "    else:\n",
    "        ax.text(0.98, 0.02, \"capacity constraint: NOT FOUND\", transform=ax.transAxes, fontsize=FONT_ANN,\n",
    "                ha='right', va='bottom', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = OUT_DIR / f\"instance_{re.sub(r'[^0-9A-Za-z_-]','_',inst)}_capacity.png\"\n",
    "    fig.savefig(fname, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    saved.append(str(fname))\n",
    "\n",
    "print(f\"Saved {len(saved)} capacity plots to {OUT_DIR}\")\n",
    "print(\"Saved capacity summary CSV to:\", OUT_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fc058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
